{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import music21\n",
    "from music21 import converter, note, chord, stream, tempo, key, pitch, duration, environment\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if needed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='seaborn')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "# music21 can be verbose with some warnings, suppress if necessary for cleanliness\n",
    "# warnings.filterwarnings('ignore', module='music21')\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# music21 environment configuration (if necessary, e.g., for MuseScore path for visualizations)\n",
    "# This is often needed for.show() calls to render scores, but not strictly for data extraction.\n",
    "# try:\n",
    "#     env = environment.UserSettings()\n",
    "#     # env.getSettingsPath() # to find where the settings file is\n",
    "#     # env = '/Applications/MuseScore 3.app/Contents/MacOS/mscore' # Example for macOS\n",
    "#     # env['musicxmlPath'] = '/Applications/MuseScore 3.app/Contents/MacOS/mscore' # Example for macOS\n",
    "# except music21.environment.EnvironmentException:\n",
    "#     print(\"music21 environment settings could not be configured (e.g., MuseScore path). This is usually fine for headless EDA.\")\n",
    "\n",
    "# Define base directory for the dataset\n",
    "# The user query states: \"the maestro unzipped dataset is in data/\"\n",
    "DATA_DIR = Path('../data/')\n",
    "MAESTRO_METADATA_FILE = DATA_DIR / 'maestro-v3.0.0.csv' # Common version, adjust if different\n",
    "MIDI_BASE_PATH = DATA_DIR # MIDI files are in subdirectories like data/2004/, data/2006/ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_maestro_metadata(file_path: Path) -> pd.DataFrame | None:\n",
    "    if not file_path.exists():\n",
    "        print(f\"Metadata file not found: {file_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        if file_path.suffix == '.csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.suffix == '.json':\n",
    "            # MAESTRO JSON is typically one JSON object per line, or a list of objects.\n",
    "            # Adjust if it's a single JSON object with a top-level key.\n",
    "            df = pd.read_json(file_path, lines=True if 'v1.0.0' in file_path.name else False) # v1.0.0 was lines=True\n",
    "            if 'v3.0.0.json' in file_path.name: # v3.0.0 json is not line-delimited\n",
    "                 df = pd.read_json(file_path)\n",
    "\n",
    "        else:\n",
    "            print(f\"Unsupported metadata file format: {file_path.suffix}\")\n",
    "            return None\n",
    "        \n",
    "        # Construct full MIDI path. MIDI files are in subdirectories named by year,\n",
    "        # e.g., data/2004/MIDI-Unprocessed_SMF_02_R1_2004_01-04_ORIG_MID--AUDIO_02_R1_2004_06_Track06_wav.midi\n",
    "        # The 'midi_filename' column in maestro-v3.0.0.csv already contains the year prefix.\n",
    "        df['midi_filepath'] = df['midi_filename'].apply(lambda x: MIDI_BASE_PATH / x)\n",
    "        \n",
    "        # Optional: Check file existence early, though this can be slow for large datasets\n",
    "        # For now, we'll assume paths are correct and handle errors during parsing.\n",
    "        # df['midi_exists'] = df['midi_filepath'].apply(lambda x: x.exists())\n",
    "        # num_listed = len(df)\n",
    "        # num_found = df['midi_exists'].sum()\n",
    "        # if num_found < num_listed:\n",
    "        #     print(f\"Warning: {num_listed - num_found} MIDI files listed in metadata not found on disk.\")\n",
    "        # df = df[df['midi_exists']].copy() # Filter for existing files to prevent issues\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "maestro_df = load_maestro_metadata(MAESTRO_METADATA_FILE)\n",
    "\n",
    "if maestro_df is not None:\n",
    "    print(\"MAESTRO Metadata Loaded Successfully:\")\n",
    "    print(maestro_df.head())\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    maestro_df.info()\n",
    "    print(f\"\\nTotal pieces in metadata: {len(maestro_df)}\")\n",
    "    # A quick check for actual file existence for a small sample if needed for debugging\n",
    "    # sample_exists = maestro_df.head()['midi_filepath'].apply(lambda x: x.exists())\n",
    "    # print(f\"\\nExistence check for first 5 MIDI files:\\n{sample_exists}\")\n",
    "else:\n",
    "    print(\"Failed to load MAESTRO metadata. Some metadata-driven EDA parts will be skipped.\")\n",
    "    # Initialize an empty DataFrame to prevent errors in later cells if they expect maestro_df\n",
    "    maestro_df = pd.DataFrame(columns=['canonical_composer', 'canonical_title', 'split', 'year', 'midi_filename', 'duration', 'midi_filepath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_maestro_metadata(file_path: Path) -> pd.DataFrame | None:\n",
    "    if not file_path.exists():\n",
    "        print(f\"Metadata file not found: {file_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        if file_path.suffix == '.csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.suffix == '.json':\n",
    "            # MAESTRO JSON is typically one JSON object per line, or a list of objects.\n",
    "            # Adjust if it's a single JSON object with a top-level key.\n",
    "            df = pd.read_json(file_path, lines=True if 'v1.0.0' in file_path.name else False) # v1.0.0 was lines=True\n",
    "            if 'v3.0.0.json' in file_path.name: # v3.0.0 json is not line-delimited\n",
    "                 df = pd.read_json(file_path)\n",
    "\n",
    "        else:\n",
    "            print(f\"Unsupported metadata file format: {file_path.suffix}\")\n",
    "            return None\n",
    "        \n",
    "        # Construct full MIDI path. MIDI files are in subdirectories named by year,\n",
    "        # e.g., data/2004/MIDI-Unprocessed_SMF_02_R1_2004_01-04_ORIG_MID--AUDIO_02_R1_2004_06_Track06_wav.midi\n",
    "        # The 'midi_filename' column in maestro-v3.0.0.csv already contains the year prefix.\n",
    "        df['midi_filepath'] = df['midi_filename'].apply(lambda x: MIDI_BASE_PATH / x)\n",
    "        \n",
    "        # Optional: Check file existence early, though this can be slow for large datasets\n",
    "        # For now, we'll assume paths are correct and handle errors during parsing.\n",
    "        # df['midi_exists'] = df['midi_filepath'].apply(lambda x: x.exists())\n",
    "        # num_listed = len(df)\n",
    "        # num_found = df['midi_exists'].sum()\n",
    "        # if num_found < num_listed:\n",
    "        #     print(f\"Warning: {num_listed - num_found} MIDI files listed in metadata not found on disk.\")\n",
    "        # df = df[df['midi_exists']].copy() # Filter for existing files to prevent issues\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "maestro_df = load_maestro_metadata(MAESTRO_METADATA_FILE)\n",
    "\n",
    "if maestro_df is not None:\n",
    "    print(\"MAESTRO Metadata Loaded Successfully:\")\n",
    "    print(maestro_df.head())\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    maestro_df.info()\n",
    "    print(f\"\\nTotal pieces in metadata: {len(maestro_df)}\")\n",
    "    # A quick check for actual file existence for a small sample if needed for debugging\n",
    "    # sample_exists = maestro_df.head()['midi_filepath'].apply(lambda x: x.exists())\n",
    "    # print(f\"\\nExistence check for first 5 MIDI files:\\n{sample_exists}\")\n",
    "else:\n",
    "    print(\"Failed to load MAESTRO metadata. Some metadata-driven EDA parts will be skipped.\")\n",
    "    # Initialize an empty DataFrame to prevent errors in later cells if they expect maestro_df\n",
    "    maestro_df = pd.DataFrame(columns=['canonical_composer', 'canonical_title', 'split', 'year', 'midi_filename', 'duration', 'midi_filepath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if maestro_df is not None and not maestro_df.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(maestro_df['year'], \n",
    "                 bins=len(maestro_df['year'].unique()) if maestro_df['year'].nunique() > 0 else 1, \n",
    "                 kde=False)\n",
    "    plt.title('Distribution of Pieces by Performance Year')\n",
    "    plt.xlabel('Year of Performance')\n",
    "    plt.ylabel('Number of Pieces')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(maestro_df['duration'] / 60, bins=50, kde=True) # Duration in minutes\n",
    "    plt.title('Distribution of Piece Durations (Minutes)')\n",
    "    plt.xlabel('Duration (Minutes)')\n",
    "    plt.ylabel('Number of Pieces')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 8)) # Increased figure size for better label readability\n",
    "    top_n_composers = 20 # Show more composers\n",
    "    composer_counts = maestro_df['canonical_composer'].value_counts()\n",
    "    # Filter out composers with very few pieces for a cleaner plot, e.g., > 5 pieces\n",
    "    # composer_counts_filtered = composer_counts[composer_counts > 5]\n",
    "    # composer_counts_to_plot = composer_counts_filtered.nlargest(top_n_composers)\n",
    "    composer_counts_to_plot = composer_counts.nlargest(top_n_composers)\n",
    "    \n",
    "    sns.barplot(x=composer_counts_to_plot.index, y=composer_counts_to_plot.values, palette=\"viridis\")\n",
    "    plt.title(f'Top {len(composer_counts_to_plot)} Composers by Number of Pieces')\n",
    "    plt.xlabel('Composer')\n",
    "    plt.ylabel('Number of Pieces')\n",
    "    plt.xticks(rotation=60, ha='right') # Adjusted rotation for clarity\n",
    "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nSummary statistics for piece duration (seconds):\")\n",
    "    print(maestro_df['duration'].describe())\n",
    "else:\n",
    "    print(\"maestro_df is None or empty, skipping metadata visualizations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midi_processing import process_single_midi_file\n",
    "\n",
    "with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "    results = list(tqdm(pool.imap_unordered(process_single_midi_file, maestro_df['midi_filepath']), total=len(maestro_df)))\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if maestro_df is not None and not maestro_df.empty and not results_df.empty:\n",
    "    maestro_df['midi_filename_basename'] = maestro_df['midi_filename'].apply(lambda x: Path(x).name)\n",
    "    results_df.rename(columns={'midi_filename_processed': 'midi_filename_basename'}, inplace=True)\n",
    "    \n",
    "    combined_df = pd.merge(maestro_df, results_df, on='midi_filename_basename', how='inner', suffixes=('_meta', '_extracted'))\n",
    "    print(f\"Combined DataFrame after merging with metadata (shape: {combined_df.shape}):\")\n",
    "elif not results_df.empty:\n",
    "    combined_df = results_df\n",
    "    print(\"Using features extracted directly from MIDI files (metadata not merged).\")\n",
    "    print(f\"Features DataFrame shape: {combined_df.shape}\")\n",
    "else:\n",
    "    combined_df = pd.DataFrame() \n",
    "    print(\"Resulting combined_df is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv('combined_df.csv') \n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty and 'key_tonic' in combined_df.columns and 'key_mode' in combined_df.columns:\n",
    "    combined_df['key_tonic_str'] = combined_df['key_tonic'].astype(str).fillna('Unknown')\n",
    "    combined_df['key_mode_str'] = combined_df['key_mode'].astype(str).fillna('')\n",
    "    \n",
    "    combined_df['estimated_key_full'] = combined_df['key_tonic_str'] + ' ' + combined_df['key_mode_str']\n",
    "    combined_df['estimated_key_full'] = combined_df['estimated_key_full'].str.replace('Unknown ', 'Unknown', regex=False).str.strip()\n",
    "\n",
    "    valid_keys_df = combined_df[~combined_df['estimated_key_full'].isin(['Unknown', 'None None'])]\n",
    "\n",
    "    if not valid_keys_df.empty:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        key_counts = valid_keys_df['estimated_key_full'].value_counts().nlargest(24)\n",
    "        sns.barplot(x=key_counts.index, y=key_counts.values, palette=\"crest\")\n",
    "        plt.title('Distribution of Estimated Keys (Top 24)')\n",
    "        plt.xlabel('Estimated Key')\n",
    "        plt.ylabel('Number of Pieces')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        mode_counts = valid_keys_df['key_mode_str'].value_counts()\n",
    "        if 'major' not in mode_counts.index and 'minor' in mode_counts.index : mode_counts.loc['major'] = 0\n",
    "        if 'minor' not in mode_counts.index and 'major' in mode_counts.index : mode_counts.loc['minor'] = 0\n",
    "        mode_counts = mode_counts.sort_index()\n",
    "\n",
    "        sns.barplot(x=mode_counts.index, y=mode_counts.values, palette=\"pastel\")\n",
    "        plt.title('Distribution of Modes (Major/Minor)')\n",
    "        plt.xlabel('Mode')\n",
    "        plt.ylabel('Number of Pieces')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nFrequency of Top Estimated Keys:\")\n",
    "        print(key_counts)\n",
    "        print(\"\\nFrequency of Modes:\")\n",
    "        print(mode_counts)\n",
    "        \n",
    "        key_freq_table_data = key_counts.reset_index()\n",
    "        key_freq_table_data.columns = ['Estimated Key', 'Count']\n",
    "        key_freq_table_data['Percentage'] = (key_freq_table_data['Count'] / len(valid_keys_df)) * 100\n",
    "        print(\"\\nTable: Frequency of Top N Estimated Keys\")\n",
    "        print(key_freq_table_data.to_string(index=False))\n",
    "\n",
    "    else:\n",
    "        print(\"No valid key analysis results to display.\")\n",
    "else:\n",
    "    print(\"Key features ('key_tonic', 'key_mode') not available in combined_df for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty and 'pitch_min' in combined_df.columns and combined_df['pitch_min'].notna().all():\n",
    "\n",
    "    if 'pitch_range' in combined_df.columns and combined_df['pitch_range'].notna().any():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(combined_df['pitch_range'].dropna().astype(float), bins=30, kde=True, color=\"skyblue\")\n",
    "        plt.title('Distribution of Pitch Ranges (in semitones) Per Piece')\n",
    "        plt.xlabel('Pitch Range (Semitones)')\n",
    "        plt.ylabel('Number of Pieces')\n",
    "        plt.show()\n",
    "\n",
    "    if 'pitch_mean' in combined_df.columns and combined_df['pitch_mean'].notna().any():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(combined_df['pitch_mean'].dropna().astype(float), bins=30, kde=True, color=\"salmon\")\n",
    "        plt.title('Distribution of Average MIDI Pitch Per Piece')\n",
    "        plt.xlabel('Average MIDI Pitch (C4=60)')\n",
    "        plt.ylabel('Number of Pieces')\n",
    "        plt.axvline(60, color='k', linestyle='--', label='C4 (Middle C)')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    if 'pitch_min' in combined_df.columns and combined_df['pitch_min'].notna().any():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(combined_df['pitch_min'].dropna().astype(float), bins=30, kde=True, color=\"lightgreen\", label='Min Pitch')\n",
    "        plt.title('Distribution of Minimum MIDI Pitches Per Piece')\n",
    "        plt.xlabel('Minimum MIDI Pitch')\n",
    "        plt.ylabel('Number of Pieces')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    if 'pitch_max' in combined_df.columns and combined_df['pitch_max'].notna().any():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(combined_df['pitch_max'].dropna().astype(float), bins=30, kde=True, color=\"gold\", label='Max Pitch')\n",
    "        plt.title('Distribution of Maximum MIDI Pitches Per Piece')\n",
    "        plt.xlabel('Maximum MIDI Pitch')\n",
    "        plt.ylabel('Number of Pieces')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\nSummary statistics for per-piece pitch features:\")\n",
    "    pitch_summary_cols = ['num_notes', 'pitch_min', 'pitch_max', 'pitch_mean', 'pitch_median', 'pitch_std', 'pitch_range']\n",
    "    existing_pitch_summary_cols = [col for col in pitch_summary_cols if col in combined_df.columns]\n",
    "    if existing_pitch_summary_cols:\n",
    "        pitch_stats_df = combined_df[existing_pitch_summary_cols].describe()\n",
    "        print(pitch_stats_df)\n",
    "        \n",
    "        print(\"\\nTable: Summary Statistics for Per-Piece Pitch Features\")\n",
    "        print(pitch_stats_df.to_string())\n",
    "    else:\n",
    "        print(\"No pitch summary columns available for describe().\")\n",
    "\n",
    "    # For a global pitch class distribution, one would need to aggregate all pitch events.\n",
    "    # This is computationally more intensive if not done during initial parsing.\n",
    "    # Example (if all_pitches_global was populated from samples or full data):\n",
    "    # all_pitches_global = # Placeholder: populate this by iterating through all notes in all scores\n",
    "    # if all_pitches_global:\n",
    "    #    all_pitch_classes = [int(round(p)) % 12 for p in all_pitches_global]\n",
    "    #    pc_counts = Counter(all_pitch_classes)\n",
    "    #    pc_names =\n",
    "    #    plt.figure(figsize=(10, 6))\n",
    "    #    sns.barplot(x=[pc_names[i] for i in sorted(pc_counts.keys())], y=[pc_counts[i] for i in sorted(pc_counts.keys())])\n",
    "    #    plt.title('Overall Pitch Class Distribution (Sampled/Full)')\n",
    "    #    plt.xlabel('Pitch Class')\n",
    "    #    plt.ylabel('Frequency')\n",
    "    #    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Pitch features ('pitch_min', etc.) not available in combined_df for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty and 'durations_ql_sample' in combined_df.columns and 'iois_ql_sample' in combined_df.columns:\n",
    "    all_durations_ql = []\n",
    "    for i, row in combined_df.iterrows():\n",
    "        if isinstance(row['durations_ql_sample'], list):\n",
    "            all_durations_ql.extend(row['durations_ql_sample'])\n",
    "\n",
    "    all_iois_ql = []\n",
    "    for i, row in combined_df.iterrows():\n",
    "        if isinstance(row['iois_ql_sample'], list):\n",
    "            all_iois_ql.extend(row['iois_ql_sample'])\n",
    "    \n",
    "    all_durations_ql = [d for d in all_durations_ql if d > 1e-6]\n",
    "    all_iois_ql = [i for i in all_iois_ql if i > 1e-6]\n",
    "\n",
    "    common_qls = {\n",
    "        '1/64': 0.0625, '1/32': 0.125, '1/16': 0.25, 'triplet 8th (approx)': 1/3, \n",
    "        '8th': 0.5, 'dotted 8th': 0.75, 'quarter': 1.0, \n",
    "        'triplet quarter (approx)': 2/3, 'dotted quarter': 1.5, 'half': 2.0, \n",
    "        'dotted half': 3.0, 'whole': 4.0\n",
    "    }\n",
    "\n",
    "    if all_durations_ql:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        min_dur, max_dur = np.min(all_durations_ql), np.max(all_durations_ql)\n",
    "        log_bins = np.logspace(np.log10(min_dur if min_dur > 0 else 0.01), np.log10(max_dur if max_dur > 0 else 4.0), 75)\n",
    "        sns.histplot(all_durations_ql, bins=log_bins, kde=False, color='teal')\n",
    "        plt.xscale('log')\n",
    "        plt.title('Overall Distribution of Note/Rest Durations (Quarter Lengths, Log Scale)')\n",
    "        plt.xlabel('Duration (Quarter Lengths)')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        for name, val in common_qls.items():\n",
    "            if min_dur < val < max_dur:\n",
    "                plt.axvline(val, color='r', linestyle='--', alpha=0.6, label=f'{name} ({val:.3f})')\n",
    "        plt.legend(fontsize='small', ncol=2)\n",
    "        plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        duration_bins = sorted(list(set([0, 0.0625, 0.125, 0.25, 1/3, 0.5, 2/3, 0.75, 1.0, 1.5, 2.0, 3.0, 4.0, 8.0, float('inf')])))\n",
    "        duration_labels = [\n",
    "            '0 (<1/64)',  # Or 'Almost Zero' for [0, 0.0625)\n",
    "            '1/64 (0.0625-0.125)',\n",
    "            '1/32 (0.125-0.25)',\n",
    "            '1/16 (0.25-1/3)',\n",
    "            'triplet 8th (1/3-0.5)',\n",
    "            '8th (0.5-2/3)',\n",
    "            'triplet qtr / dotted 8th (2/3-0.75)', # Bin [2/3, 0.75)\n",
    "            'dotted 8th / quarter (0.75-1.0)',   # Bin [0.75, 1.0)\n",
    "            'quarter / dotted qtr (1.0-1.5)',    # Bin [1.0, 1.5)\n",
    "            'dotted quarter / half (1.5-2.0)',   # Bin [1.5, 2.0)\n",
    "            'half / dotted half (2.0-3.0)',      # Bin [2.0, 3.0)\n",
    "            'dotted half / whole (3.0-4.0)',     # Bin [3.0, 4.0)\n",
    "            'whole (4.0-8.0)',                   # Bin [4.0, 8.0)\n",
    "            '> 2 wholes (>8.0)'                  # Label for the last bin [8.0, inf)\n",
    "        ]\n",
    "\n",
    "\n",
    "        \n",
    "        quantized_durs = pd.cut([d for d in all_durations_ql if d > 0], bins=duration_bins, labels=duration_labels, right=False, include_lowest=True)\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        quantized_durs.value_counts().plot(kind='bar', color='lightcoral')\n",
    "        plt.title('Distribution of Quantized Note/Rest Durations')\n",
    "        plt.xlabel('Quantized Duration Interval (Quarter Length)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        quantized_dur_counts = quantized_durs.value_counts().nlargest(10)\n",
    "        quantized_dur_table = quantized_dur_counts.reset_index()\n",
    "        quantized_dur_table.columns = ['Quantized Duration Interval', 'Count']\n",
    "        quantized_dur_table['Percentage'] = (quantized_dur_table['Count'] / len(quantized_durs)) * 100\n",
    "        print(\"\\nTable: Top 10 Most Frequent Quantized Note/Rest Durations\")\n",
    "        print(quantized_dur_table.to_string(index=False))\n",
    "\n",
    "    else:\n",
    "        print(\"No duration data to plot.\")\n",
    "\n",
    "    if all_iois_ql:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        min_ioi, max_ioi = np.min(all_iois_ql), np.max(all_iois_ql)\n",
    "        log_bins_ioi = np.logspace(np.log10(min_ioi if min_ioi > 0 else 0.01), np.log10(max_ioi if max_ioi > 0 else 4.0), 75)\n",
    "        sns.histplot(all_iois_ql, bins=log_bins_ioi, kde=False, color='darkorange')\n",
    "        plt.xscale('log')\n",
    "        plt.title('Overall Distribution of Inter-Onset Intervals (Quarter Lengths, Log Scale)')\n",
    "        plt.xlabel('IOI (Quarter Lengths)')\n",
    "        plt.ylabel('Frequency')\n",
    "        for name, val in common_qls.items():\n",
    "            if min_ioi < val < max_ioi:\n",
    "                plt.axvline(val, color='b', linestyle=':', alpha=0.6, label=f'{name} ({val:.3f})')\n",
    "        plt.legend(fontsize='small', ncol=2)\n",
    "        plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No IOI data to plot.\")\n",
    "\n",
    "    rhythm_summary_cols = ['avg_duration_ql', 'median_duration_ql', 'std_duration_ql', 'num_rhythmic_elements',\n",
    "                           'avg_ioi_ql', 'median_ioi_ql', 'std_ioi_ql', 'num_iois']\n",
    "    existing_rhythm_summary_cols = [col for col in rhythm_summary_cols if col in combined_df.columns]\n",
    "    if existing_rhythm_summary_cols and not combined_df[existing_rhythm_summary_cols].dropna().empty:\n",
    "        print(\"\\nSummary statistics for per-piece rhythmic features (averages, counts):\")\n",
    "        print(combined_df[existing_rhythm_summary_cols].describe())\n",
    "    else:\n",
    "        print(\"Not enough per-piece rhythmic summary data for description.\")\n",
    "else:\n",
    "    print(\"Rhythmic features ('durations_ql_sample', 'iois_ql_sample') not available in combined_df for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty and 'initial_tempo_bpm' in combined_df.columns:\n",
    "    valid_initial_tempos = combined_df['initial_tempo_bpm'].dropna()\n",
    "    valid_initial_tempos = valid_initial_tempos[valid_initial_tempos > 0]\n",
    "\n",
    "    if not valid_initial_tempos.empty:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(valid_initial_tempos, bins=40, kde=True, color=\"purple\")\n",
    "        plt.title('Distribution of Initial Tempos (BPM)')\n",
    "        plt.xlabel('Tempo (BPM)')\n",
    "        plt.ylabel('Number of Pieces')\n",
    "        tempo_markings = {'Largo': 50, 'Adagio': 70, 'Andante': 90, 'Moderato': 110, 'Allegro': 130, 'Presto': 180}\n",
    "        for mark, bpm_val in tempo_markings.items():\n",
    "            if valid_initial_tempos.min() < bpm_val < valid_initial_tempos.max():\n",
    "                 plt.axvline(bpm_val, color='gray', linestyle='--', alpha=0.8, label=f'{mark} ({bpm_val} BPM)')\n",
    "        plt.legend(fontsize='small')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nSummary statistics for initial tempo (BPM):\")\n",
    "        print(valid_initial_tempos.describe())\n",
    "    else:\n",
    "        print(\"No valid initial tempo data to plot after filtering.\")\n",
    "    \n",
    "    if 'num_distinct_tempos' in combined_df.columns:\n",
    "        num_tempo_changes_series = combined_df['num_distinct_tempos'].dropna().astype(int)\n",
    "        pieces_with_tempo_changes = num_tempo_changes_series[num_tempo_changes_series > 1]\n",
    "        \n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.histplot(num_tempo_changes_series, discrete=True, stat=\"count\", shrink=0.8) \n",
    "        plt.title('Distribution of Number of Distinct Tempos within Pieces')\n",
    "        plt.xlabel('Number of Distinct Tempos (1 implies stable or single marked tempo)')\n",
    "        plt.ylabel('Number of Pieces')\n",
    "        if not num_tempo_changes_series.empty:\n",
    "             plt.xticks(sorted(num_tempo_changes_series.unique()))\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nNumber of pieces with more than one distinct tempo marking: {len(pieces_with_tempo_changes)}\")\n",
    "        if len(pieces_with_tempo_changes) > 0:\n",
    "            print(\"Examples of pieces with multiple distinct tempos:\")\n",
    "            display_cols = ['midi_filename_meta', 'num_distinct_tempos']\n",
    "            if 'all_tempos_bpm_sample' in combined_df.columns:\n",
    "                display_cols.append('all_tempos_bpm_sample')\n",
    "            \n",
    "            print(combined_df[combined_df['num_distinct_tempos'] > 1][display_cols].head())\n",
    "else:\n",
    "    print(\"Tempo features ('initial_tempo_bpm', 'num_distinct_tempos') not available in combined_df for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty and 'avg_polyphony' in combined_df.columns and 'max_polyphony' in combined_df.columns:\n",
    "    valid_avg_polyphony = combined_df['avg_polyphony'].dropna()\n",
    "    valid_avg_polyphony = valid_avg_polyphony[valid_avg_polyphony >= 0]\n",
    "\n",
    "    valid_max_polyphony = combined_df['max_polyphony'].dropna()\n",
    "    valid_max_polyphony = valid_max_polyphony[valid_max_polyphony >= 0]\n",
    "\n",
    "    if not valid_avg_polyphony.empty:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(valid_avg_polyphony, bins=30, kde=True, color=\"green\")\n",
    "        plt.title('Distribution of Average Polyphony Per Piece')\n",
    "        plt.xlabel('Average Number of Simultaneous Notes/Pitches')\n",
    "        plt.ylabel('Number of Pieces')\n",
    "        plt.show()\n",
    "        print(\"\\nSummary statistics for average polyphony:\")\n",
    "        print(valid_avg_polyphony.describe())\n",
    "\n",
    "    if not valid_max_polyphony.empty:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(valid_max_polyphony, bins=max(1, int(valid_max_polyphony.max()) - int(valid_max_polyphony.min())), kde=False, color=\"orange\", discrete=True if valid_max_polyphony.nunique()<30 else False)\n",
    "        plt.title('Distribution of Maximum Polyphony Per Piece')\n",
    "        plt.xlabel('Maximum Number of Simultaneous Notes/Pitches')\n",
    "        plt.ylabel('Number of Pieces')\n",
    "        if not valid_max_polyphony.empty:\n",
    "            plt.xticks(np.arange(int(valid_max_polyphony.min()), int(valid_max_polyphony.max())+1, step=max(1, int(valid_max_polyphony.max())//10)))\n",
    "        plt.show()\n",
    "        print(\"\\nSummary statistics for maximum polyphony:\")\n",
    "        print(valid_max_polyphony.describe())\n",
    "    \n",
    "    all_polyphony_levels = []\n",
    "    if 'polyphony_levels_sample' in combined_df.columns:\n",
    "        for poly_list in combined_df['polyphony_levels_sample'].dropna():\n",
    "            if isinstance(poly_list, list):\n",
    "                all_polyphony_levels.extend(poly_list)\n",
    "    \n",
    "    all_polyphony_levels = [p for p in all_polyphony_levels if p > 0]\n",
    "\n",
    "    if all_polyphony_levels:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        poly_counts = pd.Series(all_polyphony_levels).value_counts().sort_index()\n",
    "        poly_counts_to_plot = poly_counts[poly_counts.index <= 12]\n",
    "        \n",
    "        sns.barplot(x=poly_counts_to_plot.index, y=poly_counts_to_plot.values, color=\"cyan\")\n",
    "        plt.title('Overall Distribution of Polyphony Levels (at onsets, up to 12 voices)')\n",
    "        plt.xlabel('Number of Simultaneous Notes/Pitches')\n",
    "        plt.ylabel('Frequency of Onsets')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No aggregated polyphony level data to plot.\")\n",
    "else:\n",
    "    print(\"Polyphony features ('avg_polyphony', 'max_polyphony') not available in combined_df for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty:\n",
    "    numeric_features_for_corr = [\n",
    "        'duration', \n",
    "        'year', \n",
    "        'total_quarter_length', \n",
    "        'initial_tempo_bpm', \n",
    "        'mean_tempo_bpm',\n",
    "        'num_distinct_tempos',\n",
    "        'num_notes',\n",
    "        'pitch_min', 'pitch_max', 'pitch_mean', 'pitch_median', 'pitch_std', 'pitch_range',\n",
    "        'avg_duration_ql', 'median_duration_ql', 'std_duration_ql', 'num_rhythmic_elements',\n",
    "        'avg_ioi_ql', 'median_ioi_ql', 'std_ioi_ql', 'num_iois',\n",
    "        'avg_polyphony', 'max_polyphony', 'median_polyphony',\n",
    "        'key_confidence'\n",
    "    ]\n",
    "    \n",
    "    existing_numeric_cols_for_corr = [col for col in numeric_features_for_corr if col in combined_df.columns and pd.api.types.is_numeric_dtype(combined_df[col])]\n",
    "    \n",
    "    if not existing_numeric_cols_for_corr or len(existing_numeric_cols_for_corr) < 2:\n",
    "        print(\"Not enough numeric features available for correlation analysis.\")\n",
    "    else:\n",
    "        corr_df_subset = combined_df[existing_numeric_cols_for_corr].copy()\n",
    "        corr_df_subset.dropna(inplace=True)\n",
    "\n",
    "        if not corr_df_subset.empty and len(corr_df_subset.columns) > 1 and len(corr_df_subset) > 1:\n",
    "            correlation_matrix = corr_df_subset.corr()\n",
    "            \n",
    "            plt.figure(figsize=(18, 15))\n",
    "            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, annot_kws={\"size\": 8})\n",
    "            plt.title('Correlation Matrix of Numeric Musical Features', fontsize=16)\n",
    "            plt.xticks(fontsize=10)\n",
    "            plt.yticks(fontsize=10)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            if 'total_quarter_length' in corr_df_subset.columns and 'pitch_range' in corr_df_subset.columns:\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.scatterplot(data=corr_df_subset, x='total_quarter_length', y='pitch_range', alpha=0.4, color='blue')\n",
    "                sns.regplot(data=corr_df_subset, x='total_quarter_length', y='pitch_range', scatter=False, color='red')\n",
    "                plt.title('Piece Length (Total QL) vs. Pitch Range')\n",
    "                plt.xlabel('Total Quarter Length')\n",
    "                plt.ylabel('Pitch Range (Semitones)')\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "\n",
    "            if 'initial_tempo_bpm' in corr_df_subset.columns and 'avg_ioi_ql' in corr_df_subset.columns:\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.scatterplot(data=corr_df_subset, x='initial_tempo_bpm', y='avg_ioi_ql', alpha=0.4, color='green')\n",
    "                sns.regplot(data=corr_df_subset, x='initial_tempo_bpm', y='avg_ioi_ql', scatter=False, color='red')\n",
    "                plt.title('Initial Tempo (BPM) vs. Average Inter-Onset Interval (QL)')\n",
    "                plt.xlabel('Initial Tempo (BPM)')\n",
    "                plt.ylabel('Average IOI (Quarter Lengths)')\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "                \n",
    "            if 'avg_polyphony' in corr_df_subset.columns and 'num_notes' in corr_df_subset.columns:\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.scatterplot(data=corr_df_subset, x='num_notes', y='avg_polyphony', alpha=0.4, color='purple')\n",
    "                sns.regplot(data=corr_df_subset, x='num_notes', y='avg_polyphony', scatter=False, color='red')\n",
    "                plt.title('Number of Notes vs. Average Polyphony')\n",
    "                plt.xlabel('Total Number of Notes in Piece')\n",
    "                plt.ylabel('Average Polyphony')\n",
    "                plt.xscale('log')\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "\n",
    "        else:\n",
    "            print(\"Not enough data or numeric columns for correlation analysis after cleaning NaNs.\")\n",
    "else:\n",
    "    print(\"Combined DataFrame is empty. Cannot perform correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Symbolic Music Generation Unconditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import music21\n",
    "from music21 import converter, note, chord, stream, instrument, tempo\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing\n",
    "# tqdm.auto\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='music21')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning) # General FutureWarnings\n",
    "# print torch GPU available\n",
    "print(torch.cuda.is_available())\n",
    "# --- Configuration ---\n",
    "DATA_DIR = Path('../data/')  # Adjust if your data is elsewhere\n",
    "MAESTRO_METADATA_FILE = DATA_DIR / 'maestro-v3.0.0.csv'\n",
    "MIDI_BASE_PATH = DATA_DIR # MIDI files are in subdirectories like data/2004/\n",
    "\n",
    "# Model and Generation Parameters\n",
    "MARKOV_ORDER = 3  # Order of the Markov chain\n",
    "LSTM_SEQUENCE_LENGTH = 50\n",
    "LSTM_EMBEDDING_DIM = 128\n",
    "LSTM_UNITS = 256\n",
    "LSTM_EPOCHS = 15 # Adjust based on dataset size and convergence\n",
    "LSTM_BATCH_SIZE = 2048\n",
    "GENERATION_LENGTH = 1000 # Number of events to generate\n",
    "QUANTIZED_DURATIONS_QN = np.array([\n",
    "    0.0625,  # 64th note\n",
    "    0.125,   # 32nd note\n",
    "    0.1666,  # Approx. 16th note triplet component (1/6)\n",
    "    0.25,    # 16th note\n",
    "    0.3333,  # Approx. 8th note triplet component (1/3)\n",
    "    0.5,     # 8th note\n",
    "    0.6666,  # Approx. Quarter note triplet component (2/3)\n",
    "    0.75,    # Dotted 8th note\n",
    "    1.0,     # Quarter note\n",
    "    1.5,     # Dotted Quarter note\n",
    "    2.0,     # Half note\n",
    "    3.0,     # Dotted Half note\n",
    "    4.0      # Whole note\n",
    "])\n",
    "\n",
    "def get_closest_quantized_duration(actual_ql: float) -> float:\n",
    "    \"\"\"Finds the closest duration from our predefined QUANTIZED_DURATIONS_QN list.\"\"\"\n",
    "    # Handle rests or very short notes gracefully, map them to the smallest positive duration\n",
    "    if actual_ql <= 0:\n",
    "        return QUANTIZED_DURATIONS_QN[0]\n",
    "    idx = np.abs(QUANTIZED_DURATIONS_QN - actual_ql).argmin()\n",
    "    return QUANTIZED_DURATIONS_QN[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_maestro_metadata(file_path: Path) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads MAESTRO metadata, adapted from the provided PDF.\"\"\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"Metadata file not found: {file_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Construct full MIDI path [cite: 7]\n",
    "        df['midi_filepath'] = df['midi_filename'].apply(lambda x: MIDI_BASE_PATH / x)\n",
    "        # Basic check for file existence (optional, can be slow)\n",
    "        # df['midi_exists'] = df['midi_filepath'].apply(lambda x: x.exists())\n",
    "        # print(f\"Found {df['midi_exists'].sum()} of {len(df)} MIDI files on disk.\")\n",
    "        # df = df[df['midi_exists']]\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_events_from_midi(midi_path: Path) -> list[str] | None:\n",
    "    \"\"\"Extracts notes/chords and their quantized durations as string events.\"\"\"\n",
    "    events = []\n",
    "    try:\n",
    "        score = converter.parse(midi_path)\n",
    "        notes_to_parse = score.flat.notesAndRests # Include rests if desired, or stick to .notes\n",
    "        \n",
    "        for element in notes_to_parse:\n",
    "            # Skip elements with no duration or zero duration if they cause issues\n",
    "            if element.duration is None or element.duration.quarterLength == 0:\n",
    "                continue\n",
    "\n",
    "            quantized_ql = get_closest_quantized_duration(element.duration.quarterLength)\n",
    "            \n",
    "            if isinstance(element, note.Note):\n",
    "                # Event format: \"pitch_duration\" e.g., \"60_0.5\"\n",
    "                events.append(f\"{element.pitch.midi}_{quantized_ql}\")\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                # Event format: \"pitch1.pitch2.pitch3_duration\" e.g., \"60.64.67_1.0\"\n",
    "                chord_pitches_str = '.'.join(str(p.midi) for p in sorted(element.pitches))\n",
    "                events.append(f\"{chord_pitches_str}_{quantized_ql}\")\n",
    "            # Optionally handle rests:\n",
    "            # elif isinstance(element, note.Rest):\n",
    "            #     events.append(f\"Rest_{quantized_ql}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Could not parse {midi_path}: {e}\")\n",
    "        return None\n",
    "    return events\n",
    "\n",
    "def create_vocabulary(all_event_sequences: list[list[str]]):\n",
    "    \"\"\"Creates event_to_int and int_to_event mappings.\"\"\"\n",
    "    all_events = [event for seq in all_event_sequences for event in seq]\n",
    "    event_counts = Counter(all_events)\n",
    "    unique_events = sorted(event_counts.keys()) # Sort for consistency\n",
    "    event_to_int = {event: i for i, event in enumerate(unique_events)}\n",
    "    int_to_event = {i: event for i, event in enumerate(unique_events)}\n",
    "    return event_to_int, int_to_event, len(unique_events)\n",
    "\n",
    "# --- 2. MIDI Generation Utility ---\n",
    "\n",
    "def create_midi_from_events(events: list[str], output_filename: str): # Removed default_duration_qn\n",
    "    \"\"\"Creates a MIDI file from a list of string events (pitch/chord_duration).\"\"\"\n",
    "    output_stream = stream.Stream()\n",
    "    output_stream.append(instrument.Piano())\n",
    "    bpm = 120 # Default tempo, can also be learned or configured\n",
    "    mm = tempo.MetronomeMark(number=bpm)\n",
    "    output_stream.append(mm)\n",
    "\n",
    "    # current_offset = 0.0 # If precise offsets are needed based on appending.\n",
    "                           # For simple sequential generation, music21 handles appending.\n",
    "\n",
    "    for event_str in events:\n",
    "        try:\n",
    "            parts = event_str.split('_')\n",
    "            if len(parts) != 2:\n",
    "                # print(f\"Skipping malformed event: {event_str}\")\n",
    "                continue # Skip malformed events\n",
    "\n",
    "            item_str = parts[0]\n",
    "            duration_ql_str = parts[1]\n",
    "            \n",
    "            try:\n",
    "                element_duration_ql = float(duration_ql_str)\n",
    "            except ValueError:\n",
    "                # print(f\"Skipping event with unparsable duration: {event_str}\")\n",
    "                continue\n",
    "            \n",
    "            element_duration = music21.duration.Duration(element_duration_ql)\n",
    "\n",
    "            if \"Rest\" in item_str: # Handling rests if they are in vocabulary\n",
    "                r = note.Rest(duration=element_duration)\n",
    "                output_stream.append(r)\n",
    "            elif '.' in item_str: # Chord\n",
    "                pitch_strings = item_str.split('.')\n",
    "                pitches = [int(p_str) for p_str in pitch_strings]\n",
    "                c = chord.Chord(pitches, duration=element_duration)\n",
    "                output_stream.append(c)\n",
    "            elif item_str.isdigit(): # Note\n",
    "                pitch = int(item_str)\n",
    "                n = note.Note(pitch, duration=element_duration)\n",
    "                output_stream.append(n)\n",
    "            else:\n",
    "                # print(f\"Skipping unknown event type: {item_str}\")\n",
    "                continue\n",
    "            \n",
    "            # output_stream.append(m21_event)\n",
    "            # current_offset += element_duration_ql # Advance offset if managing manually\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(f\"Error processing event '{event_str}': {e}\")\n",
    "            continue\n",
    "            \n",
    "    try:\n",
    "        output_stream.write('midi', fp=output_filename)\n",
    "        print(f\"Generated MIDI file: {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing MIDI file {output_filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChain:\n",
    "    def __init__(self, order=1):\n",
    "        self.order = order\n",
    "        self.transitions = defaultdict(Counter)\n",
    "        self.initial_states = Counter() # For generating the first 'order' states\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def train(self, sequences: list[list[int]], vocab_size: int):\n",
    "        self.vocab_size = vocab_size\n",
    "        for seq in sequences:\n",
    "            if len(seq) <= self.order:\n",
    "                continue\n",
    "            \n",
    "            # Store initial states for generation\n",
    "            initial_context = tuple(seq[:self.order])\n",
    "            self.initial_states[initial_context] += 1\n",
    "\n",
    "            for i in range(len(seq) - self.order):\n",
    "                context = tuple(seq[i : i + self.order])\n",
    "                next_event = seq[i + self.order]\n",
    "                self.transitions[context][next_event] += 1\n",
    "        \n",
    "        # Normalize initial_states for generation\n",
    "        total_initials = sum(self.initial_states.values())\n",
    "        if total_initials > 0:\n",
    "             for context in self.initial_states:\n",
    "                self.initial_states[context] /= total_initials\n",
    "\n",
    "\n",
    "    def get_prob(self, context: tuple[int, ...], next_event: int):\n",
    "        \"\"\"Calculates P(next_event | context) with Laplace smoothing.\"\"\"\n",
    "        context_counts = sum(self.transitions[context].values())\n",
    "        event_in_context_count = self.transitions[context][next_event]\n",
    "        return (event_in_context_count + 1) / (context_counts + self.vocab_size)\n",
    "\n",
    "    def generate(self, length: int) -> list[int]:\n",
    "        if not self.transitions:\n",
    "            print(\"Markov chain not trained or no transitions learned.\")\n",
    "            return []\n",
    "\n",
    "        # Start with a probable initial context\n",
    "        if not self.initial_states: # Fallback if no initial states were captured\n",
    "             current_context = list(np.random.choice(list(self.transitions.keys()))) if self.transitions else []\n",
    "        else:\n",
    "            # Sample from learned initial states\n",
    "            initial_contexts_list = list(self.initial_states.keys())\n",
    "            initial_probs = [self.initial_states[ctx] for ctx in initial_contexts_list]\n",
    "            if not initial_contexts_list or sum(initial_probs) == 0: # further fallback\n",
    "                 current_context_tuple = np.random.choice(list(self.transitions.keys())) if self.transitions else tuple()\n",
    "                 current_context = list(current_context_tuple)\n",
    "            else:\n",
    "                start_idx = np.random.choice(len(initial_contexts_list), p=initial_probs)\n",
    "                current_context = list(initial_contexts_list[start_idx])\n",
    "        \n",
    "        generated_sequence = list(current_context)\n",
    "\n",
    "        for _ in range(length - self.order):\n",
    "            if tuple(current_context) not in self.transitions or not self.transitions[tuple(current_context)]:\n",
    "                # If context is unknown or has no continuations, break or pick random\n",
    "                # For simplicity, picking a random event from vocab. A better way is backoff or random context.\n",
    "                next_event = np.random.randint(0, self.vocab_size)\n",
    "                # print(f\"Warning: Unknown context {tuple(current_context)}, choosing random next event.\")\n",
    "            else:\n",
    "                possible_next_events = list(self.transitions[tuple(current_context)].keys())\n",
    "                probabilities = [self.get_prob(tuple(current_context), event) for event in possible_next_events]\n",
    "                # Normalize probabilities if they don't sum to 1 (due to smoothing in get_prob calculation)\n",
    "                prob_sum = sum(probabilities)\n",
    "                if prob_sum > 0:\n",
    "                    probabilities = [p/prob_sum for p in probabilities]\n",
    "                else: # Fallback if all probabilities are zero (should not happen with Laplace)\n",
    "                    probabilities = [1.0/len(possible_next_events)] * len(possible_next_events)\n",
    "                \n",
    "                if not possible_next_events: # Should not happen if context in transitions\n",
    "                    next_event = np.random.randint(0, self.vocab_size)\n",
    "                else:\n",
    "                    next_event = np.random.choice(possible_next_events, p=probabilities)\n",
    "\n",
    "            generated_sequence.append(next_event)\n",
    "            current_context = generated_sequence[-self.order:]\n",
    "        \n",
    "        return generated_sequence\n",
    "\n",
    "    def calculate_perplexity(self, sequences: list[list[int]]):\n",
    "        log_likelihood_sum = 0\n",
    "        total_events = 0\n",
    "        for seq in sequences:\n",
    "            if len(seq) <= self.order:\n",
    "                continue\n",
    "            for i in range(len(seq) - self.order):\n",
    "                context = tuple(seq[i : i + self.order])\n",
    "                next_event = seq[i + self.order]\n",
    "                prob = self.get_prob(context, next_event)\n",
    "                if prob > 1e-9: # Avoid log(0)\n",
    "                    log_likelihood_sum += np.log2(prob)\n",
    "                else: # Penalize heavily for zero probability events\n",
    "                    log_likelihood_sum -= 20 # Arbitrary large penalty\n",
    "                total_events += 1\n",
    "        \n",
    "        if total_events == 0: return float('inf')\n",
    "        avg_log_likelihood = log_likelihood_sum / total_events\n",
    "        perplexity = np.power(2, -avg_log_likelihood)\n",
    "        return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_for_lstm_pytorch(sequences: list[list[int]], seq_len: int):\n",
    "    X, y = [], []\n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq) - seq_len):\n",
    "            X.append(seq[i : i + seq_len])\n",
    "            y.append(seq[i + seq_len])\n",
    "    X = torch.tensor(X, dtype=torch.long)  # LongTensor for embeddings\n",
    "    y = torch.tensor(y, dtype=torch.long)  # AdaptiveLogSoftmax expects class indices\n",
    "    return X, y\n",
    "\n",
    "def build_lstm_model(vocab_size: int, embedding_dim: int, lstm_units: int, num_layers: int = 2):\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim, hidden_dim, cutoffs=[1000, 10000, 50000]):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=0.2, num_layers=num_layers)\n",
    "            self.adaptive_softmax = nn.AdaptiveLogSoftmaxWithLoss(\n",
    "                in_features=hidden_dim,\n",
    "                n_classes=vocab_size,\n",
    "                cutoffs=cutoffs\n",
    "            )\n",
    "\n",
    "        def forward(self, x, target=None):\n",
    "            x = self.embedding(x)\n",
    "            out, _ = self.lstm(x)\n",
    "            out = out[:, -1, :]  # last time step\n",
    "\n",
    "            if self.training and target is not None:\n",
    "                return self.adaptive_softmax(out, target)\n",
    "            else:\n",
    "                return out  # raw output for generation/logits\n",
    "\n",
    "    return LSTMModel(vocab_size, embedding_dim, lstm_units)\n",
    "\n",
    "def generate_sequence_lstm(model, seed_sequence: list[int], length: int, vocab_size: int, temperature=1.0, device='cpu'):\n",
    "    model.eval()\n",
    "    generated_sequence = list(seed_sequence)\n",
    "    current_input_sequence = torch.tensor(seed_sequence, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            logits = model(current_input_sequence)  # (1, hidden_dim)\n",
    "            probs = torch.softmax(model.adaptive_softmax.log_prob(logits), dim=-1).squeeze(0).cpu().numpy()\n",
    "\n",
    "            if temperature > 0:\n",
    "                scaled_probs = np.power(probs, 1.0 / temperature)\n",
    "                scaled_probs /= np.sum(scaled_probs)\n",
    "            else:\n",
    "                scaled_probs = np.zeros_like(probs)\n",
    "                scaled_probs[np.argmax(probs)] = 1.0\n",
    "\n",
    "            if np.isnan(scaled_probs).any() or np.sum(scaled_probs) == 0:\n",
    "                next_event_int = np.random.choice(vocab_size)\n",
    "            else:\n",
    "                next_event_int = np.random.choice(vocab_size, p=scaled_probs)\n",
    "\n",
    "            generated_sequence.append(next_event_int)\n",
    "            current_input_sequence = torch.cat(\n",
    "                [current_input_sequence[:, 1:], torch.tensor([[next_event_int]], device=device)], dim=1\n",
    "            )\n",
    "\n",
    "    return generated_sequence\n",
    "\n",
    "def calculate_perplexity_lstm(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            output = model(X_batch, y_batch)\n",
    "            loss = output.loss\n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "            total_tokens += y_batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    return perplexity\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "def lstm_train(model, train_loader, val_loader, epochs, device):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=5e-3,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_val_perp = float(\"inf\")\n",
    "    patience = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            X_batch = X_batch.to(device, non_blocking=True)\n",
    "            y_batch = y_batch.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                # only embed and LSTM under autocast\n",
    "                logits = model.embedding(X_batch)\n",
    "                out, _ = model.lstm(logits)\n",
    "                out = out[:, -1, :]\n",
    "            # Compute loss outside autocast (in float32)\n",
    "            output = model.adaptive_softmax(out.float(), y_batch)\n",
    "            loss = output.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val = X_val.to(device, non_blocking=True)\n",
    "                y_val = y_val.to(device, non_blocking=True)\n",
    "                out = model.embedding(X_val)\n",
    "                out, _ = model.lstm(out)\n",
    "                out = out[:, -1, :]\n",
    "                output = model.adaptive_softmax(out.float(), y_val)\n",
    "                loss = output.loss\n",
    "                val_loss += loss.item() * X_val.size(0)\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_perplexity = torch.exp(torch.tensor(avg_val_loss)).item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}  \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}  \"\n",
    "            f\"Val Perplexity: {val_perplexity:.2f}\"\n",
    "        )\n",
    "\n",
    "        if val_perplexity < best_val_perp:\n",
    "            best_val_perp = val_perplexity\n",
    "            patience = 0\n",
    "            torch.save(model.state_dict(), \"best_lstm.pt\")\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 3:\n",
    "                print(\"Early stopping (no improvement in 3 epochs).\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(\"best_lstm.pt\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MAESTRO metadata...\n",
      "Extracting musical events from MIDI files...\n",
      "Using 64 processes for parsing.\n",
      "Processing 962 files for train split using multiprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing train: 100%|| 962/962 [10:02<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 137 files for validation split using multiprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing validation: 100%|| 137/137 [02:17<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 177 files for test split using multiprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing test: 100%|| 177/177 [01:54<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary...\n",
      "Vocabulary size: 319111\n",
      "\n",
      "--- Markov Chain Model ---\n",
      "Training Markov chain (order 3)...\n",
      "Markov Chain Perplexity on Test Set: 317247.99\n",
      "Generating music with Markov chain...\n",
      "Generated MIDI file: markov_generated.mid\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading MAESTRO metadata...\")\n",
    "maestro_df = load_maestro_metadata(MAESTRO_METADATA_FILE)\n",
    "\n",
    "if maestro_df is None or maestro_df.empty:\n",
    "    print(\"Failed to load MAESTRO metadata. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Extracting musical events from MIDI files...\")\n",
    "all_sequences_str_train, all_sequences_str_val, all_sequences_str_test = [], [], []\n",
    "\n",
    "# Limiting files for faster demonstration - remove for full run\n",
    "# max_files_per_split = 50 \n",
    "num_processes = multiprocessing.cpu_count()\n",
    "print(f\"Using {num_processes} processes for parsing.\")\n",
    "\n",
    "for split_type, target_list in [('train', all_sequences_str_train), \n",
    "                                ('validation', all_sequences_str_val), \n",
    "                                ('test', all_sequences_str_test)]:\n",
    "    split_df = maestro_df[maestro_df['split'] == split_type]\n",
    "    # if max_files_per_split: \n",
    "    #    split_df = split_df.head(max_files_per_split)\n",
    "    \n",
    "    # First, get a list of MIDI paths that actually exist to avoid errors in the pool\n",
    "    existing_midi_paths = [path for path in split_df['midi_filepath'] if path.exists()]\n",
    "    \n",
    "    if len(existing_midi_paths) < len(split_df['midi_filepath']):\n",
    "        print(f\"Warning: {len(split_df['midi_filepath']) - len(existing_midi_paths)} MIDI file(s) not found for {split_type} split.\")\n",
    "\n",
    "    if not existing_midi_paths:\n",
    "        print(f\"No existing MIDI files to process for {split_type} split.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing {len(existing_midi_paths)} files for {split_type} split using multiprocessing...\")\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    with multiprocessing.Pool(processes=num_processes * 2) as pool:\n",
    "        # Use imap_unordered to get results as they complete, which can be faster\n",
    "        # if processing times vary. Wrap with tqdm for a progress bar.\n",
    "        # The extract_events_from_midi function will be called for each path in existing_midi_paths.\n",
    "        results_iterator = pool.imap_unordered(extract_events_from_midi, existing_midi_paths)\n",
    "        \n",
    "        for events in tqdm(results_iterator, total=len(existing_midi_paths), desc=f\"Parsing {split_type}\"):\n",
    "            if events and len(events) > max(MARKOV_ORDER, LSTM_SEQUENCE_LENGTH): # Ensure sequence is long enough\n",
    "                target_list.append(events)\n",
    "\n",
    "\n",
    "if not all_sequences_str_train:\n",
    "    print(\"No training data could be extracted. Check MIDI paths and parsing. Exiting.\")\n",
    "    exit()\n",
    "    \n",
    "print(\"Creating vocabulary...\")\n",
    "event_to_int, int_to_event, vocab_size = create_vocabulary(all_sequences_str_train + all_sequences_str_val + all_sequences_str_test)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Convert string sequences to integer sequences\n",
    "all_sequences_int_train = [[event_to_int[e] for e in seq if e in event_to_int] for seq in all_sequences_str_train]\n",
    "all_sequences_int_val = [[event_to_int[e] for e in seq if e in event_to_int] for seq in all_sequences_str_val]\n",
    "all_sequences_int_test = [[event_to_int[e] for e in seq if e in event_to_int] for seq in all_sequences_str_test]\n",
    "\n",
    "# Filter out empty sequences after mapping (if any event wasn't in vocab learned from train)\n",
    "all_sequences_int_train = [s for s in all_sequences_int_train if len(s) > MARKOV_ORDER]\n",
    "all_sequences_int_val = [s for s in all_sequences_int_val if len(s) > MARKOV_ORDER]\n",
    "all_sequences_int_test = [s for s in all_sequences_int_test if len(s) > MARKOV_ORDER]\n",
    "\n",
    "\n",
    "if not all_sequences_int_train:\n",
    "    print(\"No valid integer sequences for training after vocabulary mapping. Exiting.\")\n",
    "    exit()\n",
    "if not all_sequences_int_test:\n",
    "    print(\"Warning: No valid integer sequences for testing. Perplexity might not be meaningful.\")\n",
    "    # Fallback: use validation set for testing if test set is empty\n",
    "    if all_sequences_int_val:\n",
    "        print(\"Using validation set for testing perplexity as test set is empty.\")\n",
    "        all_sequences_int_test = all_sequences_int_val\n",
    "    else:\n",
    "        print(\"Both test and validation sets are empty after processing. Cannot calculate perplexity. Exiting.\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Markov Chain ---\n",
    "print(\"\\n--- Markov Chain Model ---\")\n",
    "markov_model = MarkovChain(order=MARKOV_ORDER)\n",
    "print(f\"Training Markov chain (order {MARKOV_ORDER})...\")\n",
    "markov_model.train(all_sequences_int_train, vocab_size)\n",
    "\n",
    "if all_sequences_int_test:\n",
    "    markov_perplexity = markov_model.calculate_perplexity(all_sequences_int_test)\n",
    "    print(f\"Markov Chain Perplexity on Test Set: {markov_perplexity:.2f}\")\n",
    "else:\n",
    "    print(\"No test data for Markov Chain perplexity.\")\n",
    "\n",
    "print(\"Generating music with Markov chain...\")\n",
    "if all_sequences_int_train and all_sequences_int_train[0]:\n",
    "        # Ensure there is at least one training sequence to pick a seed from\n",
    "    seed_for_markov = all_sequences_int_train[0][:MARKOV_ORDER] if len(all_sequences_int_train[0]) >= MARKOV_ORDER else \\\n",
    "                        (all_sequences_int_train[0] + [np.random.randint(0, vocab_size)] * (MARKOV_ORDER - len(all_sequences_int_train[0])))[:MARKOV_ORDER]\n",
    "\n",
    "    if len(seed_for_markov) == MARKOV_ORDER : # Markov model generates from internal state or random context if no seed\n",
    "        generated_markov_ints = markov_model.generate(length=GENERATION_LENGTH)\n",
    "        generated_markov_events = [int_to_event.get(i, str(i)) for i in generated_markov_ints] # Handle if int not in map\n",
    "        create_midi_from_events(generated_markov_events, \"markov_generated.mid\")\n",
    "    else:\n",
    "        print(\"Could not create a valid seed for Markov chain generation from training data.\")\n",
    "else:\n",
    "    print(\"No training data to seed Markov chain generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LSTM Model ---\n",
      "Preparing data for LSTM...\n",
      "LSTM training data shape: X=torch.Size([2922699, 50]), y=torch.Size([2922699])\n",
      "LSTM test data shape: X=torch.Size([367368, 50]), y=torch.Size([367368])\n",
      "First batch loaded in 0.62 seconds\n",
      "LSTMModel(\n",
      "  (embedding): Embedding(319111, 128)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (adaptive_softmax): AdaptiveLogSoftmaxWithLoss(\n",
      "    (head): Linear(in_features=256, out_features=1003, bias=False)\n",
      "    (tail): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=64, bias=False)\n",
      "        (1): Linear(in_features=64, out_features=9000, bias=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=16, bias=False)\n",
      "        (1): Linear(in_features=16, out_features=40000, bias=False)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=4, bias=False)\n",
      "        (1): Linear(in_features=4, out_features=269111, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Training LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|| 1142/1142 [02:08<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  Train Loss: 11.2574  Val Perplexity: 11488.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|| 1142/1142 [02:07<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15  Train Loss: 9.0668  Val Perplexity: 9486.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|| 1142/1142 [02:07<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15  Train Loss: 8.9034  Val Perplexity: 8609.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|| 1142/1142 [02:07<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15  Train Loss: 8.6221  Val Perplexity: 6767.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|| 1142/1142 [02:07<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15  Train Loss: 8.2623  Val Perplexity: 5589.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|| 1142/1142 [02:07<00:00,  8.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15  Train Loss: 7.9844  Val Perplexity: 5306.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|| 1142/1142 [02:07<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15  Train Loss: 7.7834  Val Perplexity: 4965.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|| 1142/1142 [02:06<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15  Train Loss: 7.6150  Val Perplexity: 4751.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|| 1142/1142 [02:07<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15  Train Loss: 7.4524  Val Perplexity: 4989.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|| 1142/1142 [02:07<00:00,  8.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15  Train Loss: 7.2899  Val Perplexity: 5633.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|| 1142/1142 [02:06<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15  Train Loss: 7.1292  Val Perplexity: 6796.34\n",
      "Early stopping (no improvement in 3 epochs).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 50\u001b[0m         lstm_perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_perplexity_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM Perplexity on Test Set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlstm_perplexity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[33], line 73\u001b[0m, in \u001b[0;36mcalculate_perplexity_lstm\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     71\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     72\u001b[0m output \u001b[38;5;241m=\u001b[39m model(X_batch, y_batch)\n\u001b[0;32m---> 73\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\n\u001b[1;32m     74\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     75\u001b[0m total_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- LSTM Model ---\")\n",
    "print(\"Preparing data for LSTM...\")\n",
    "\n",
    "# Prepare training sequences\n",
    "X_train_lstm, y_train_lstm = prepare_sequences_for_lstm_pytorch(all_sequences_int_train, LSTM_SEQUENCE_LENGTH)\n",
    "\n",
    "if X_train_lstm.size(0) == 0:\n",
    "    print(\"Not enough training data to form sequences for LSTM. Exiting LSTM part.\")\n",
    "else:\n",
    "    X_test_lstm, y_test_lstm = prepare_sequences_for_lstm_pytorch(all_sequences_int_test, LSTM_SEQUENCE_LENGTH)\n",
    "    \n",
    "    if X_test_lstm.size(0) == 0 and all_sequences_int_val:\n",
    "        print(\"Warning: Test set too small for LSTM sequence preparation. Trying with validation set for perplexity.\")\n",
    "        X_test_lstm, y_test_lstm = prepare_sequences_for_lstm_pytorch(all_sequences_int_val, LSTM_SEQUENCE_LENGTH)\n",
    "    \n",
    "    print(f\"LSTM training data shape: X={X_train_lstm.shape}, y={y_train_lstm.shape}\")\n",
    "    if X_test_lstm.size(0) > 0:\n",
    "        print(f\"LSTM test data shape: X={X_test_lstm.shape}, y={y_test_lstm.shape}\")\n",
    "    else:\n",
    "        print(\"Warning: Not enough test/validation data to form sequences for LSTM perplexity calculation.\")\n",
    "\n",
    "    # Create Dataset and DataLoader for training and testing\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_lstm, y_train_lstm)\n",
    "    from torch.utils.data import random_split\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=LSTM_BATCH_SIZE,num_workers=4, shuffle=True, pin_memory=True)\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    batch = next(iter(train_loader))\n",
    "    print(f\"First batch loaded in {time.time() - start:.2f} seconds\")\n",
    "    val_loader = DataLoader(val_dataset, batch_size=LSTM_BATCH_SIZE, num_workers=4, pin_memory=True)\n",
    "    \n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(X_test_lstm, y_test_lstm) if X_test_lstm.size(0) > 0 else None\n",
    "    test_loader = DataLoader(test_dataset, batch_size=LSTM_BATCH_SIZE, num_workers=4, pin_memory=True) if test_dataset else None\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    lstm_model = build_lstm_model(vocab_size, LSTM_EMBEDDING_DIM, LSTM_UNITS).to(device)\n",
    "    print(lstm_model)\n",
    "\n",
    "    print(\"Training LSTM model...\")\n",
    "    lstm_model = lstm_train(lstm_model, train_loader, val_loader, epochs=LSTM_EPOCHS, device=device)\n",
    "\n",
    "    if test_loader is not None:\n",
    "        with torch.no_grad():\n",
    "            lstm_perplexity = calculate_perplexity_lstm(lstm_model, test_loader, device=device)\n",
    "        print(f\"LSTM Perplexity on Test Set: {lstm_perplexity:.2f}\")\n",
    "    else:\n",
    "        print(\"LSTM perplexity cannot be calculated due to insufficient test/validation data meeting sequence length.\")\n",
    "\n",
    "    print(\"Generating music with LSTM...\")\n",
    "    if all_sequences_int_train and len(all_sequences_int_train[0]) >= LSTM_SEQUENCE_LENGTH:\n",
    "        import numpy as np\n",
    "        seed_idx_lstm = np.random.randint(0, len(all_sequences_int_train))\n",
    "        seed_start_idx_lstm = np.random.randint(0, len(all_sequences_int_train[seed_idx_lstm]) - LSTM_SEQUENCE_LENGTH)\n",
    "        seed_for_lstm = all_sequences_int_train[seed_idx_lstm][seed_start_idx_lstm : seed_start_idx_lstm + LSTM_SEQUENCE_LENGTH]\n",
    "\n",
    "        generated_lstm_ints = generate_sequence_lstm(\n",
    "            lstm_model, seed_for_lstm, GENERATION_LENGTH, vocab_size, temperature=0.7, device=device\n",
    "        )\n",
    "        generated_lstm_events = [int_to_event.get(i, str(i)) for i in generated_lstm_ints]  # Fallback if missing\n",
    "        create_midi_from_events(generated_lstm_events, \"lstm_generated.mid\")\n",
    "    else:\n",
    "        print(\"Could not find a suitable seed sequence from training data for LSTM generation.\")\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-tf-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
